{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##RNN 실습 코드 : https://wikidocs.net/64703\n",
        "#1. 문자 단위 RNN (Char RNN)\n",
        "#2. 더 많은 데이터로 학습한 문자 단위 RNN (Char RNN)"
      ],
      "metadata": {
        "id": "oUBVZHtbryor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. 문자 단위 RNN (Char RNN)"
      ],
      "metadata": {
        "id": "6B7mi5ZZtkHY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PRI02RFQruom"
      },
      "outputs": [],
      "source": [
        "# 필요한 패키지 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Step 1) 훈련 데이터셋 전처리\n",
        "\n",
        "- 입력 데이터와 레이블 데이터에 대해서 문자 집합(vocabulary)을 만듦\n",
        "- 문자 집합은 중복을 제거한 문자들의 집합\n",
        "\n"
      ],
      "metadata": {
        "id": "70ehIKqat1ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자 시퀀스 apple을 입력받으면 pple!을 출력하는 RNN 구현 (RNN 동작 이해하기 위함)\n",
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str + label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print('문자 집합의 크기: {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNN24q6PtvUF",
        "outputId": "45be081f-4a5c-43f2-f334-3cfdb70e9238"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 현재 문자 집합엔 총 5개의 문자가 있음.\n",
        "- !, a, e, l, p"
      ],
      "metadata": {
        "id": "cxtNFHmaugJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터 정의\n",
        "input_size = vocab_size # 원핫 벡터를 사용할 것이므로 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "-r3MZaOqueri"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자 집합에 고유한 정수를 부여\n",
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab))\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2A64SbLu5TT",
        "outputId": "319b23f3-b09f-4c29-990e-cf35a44e8e08"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 나중에 예측 결과를 다시 문자 시퀀스로 보기 위해서 반대로 정수로부터 문자를 얻을 수 있는 index_to_char을 만듦\n",
        "index_to_char = {}\n",
        "for key, value in char_to_index.items():\n",
        "  index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDXH00crvD0V",
        "outputId": "16ba1eac-6d60-4f05-c015-d25e6b6e6cb1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터와 레이블 데이터의 각 문자들을 정수로 매핑\n",
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glDaoYmFvRPd",
        "outputId": "1cef4ad6-beb0-49b7-c781-f8795e4ac667"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이토치 nn.RNN()은 3차원 텐서를 입력받으므로 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)을 통해서도 해결 가능\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1qUkflive0R",
        "outputId": "54dfe5d1-d2d5-4940-853f-fa22bd034f95"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 시퀀스의 각 문자들을 원핫 벡터롤 변환\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCnsRVLpvu31",
        "outputId": "4719b998-648f-4bdf-8a6d-864915239529"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터와 레이블 데이터를 텐서로 바꿔줌\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "print('훈련 데이터의 크기: {}'.format(X.shape))\n",
        "print('레이블의 크기: {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsZ22RjDv5X1",
        "outputId": "81d9687b-0fe4-48b0-a827-6f1b3dcd033c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기: torch.Size([1, 5, 5])\n",
            "레이블의 크기: torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Step 2) 모델 구현\n"
      ],
      "metadata": {
        "id": "JzpCcakVwPRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN 모델 구현\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "    self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "  def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "AQ-UGYxGwJm6"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스로 정의한 모델을 net에 저장\n",
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "k14mzA4-wvp4"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력된 모델에 입력을 넣어서 출력의 크기를 확인\n",
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubyp-YVfw1jy",
        "outputId": "22114bdc-e0ae-45f6-cb3a-126eebde36fa"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- (1,5,5)는 차례대로 (배치 차원, 시점(timesteps), 출력의 크기)를 의미\n"
      ],
      "metadata": {
        "id": "8lwpPeZOyhFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도를 측정할 땐 view를 이용하여 배치차원과 시점차원을 하나도 만듦\n",
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfLh78uYw8Sl",
        "outputId": "cebefa56-119f-4cec-f4b1-1fc0aea939c9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 차원이 (5,5)가 됨을 확인"
      ],
      "metadata": {
        "id": "5vB2h-wNy5qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 데이터의 크기 확인\n",
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBINz-tFy5Ip",
        "outputId": "5bc8c844-c005-42bd-eab7-01ee232a5ed2"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 옵티마이저와 손실함수를 정의\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "Ov-tDPe0y3Uo"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 총 100번의 에포크를 학습\n",
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X)\n",
        "  loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유응 Batch 차원 제거를 위해\n",
        "  loss.backward() # 기울기 계산\n",
        "  optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "  # 아래 세 줄을 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드\n",
        "  result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 timestep별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "  result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "  print(i, 'loss: ', loss.item(), 'prediction: ', result,\n",
        "        'true Y: ', y_data, 'prediction str: ', result_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szYNppjzzQ7P",
        "outputId": "9d627d4f-144e-4127-c12e-4baa52e96d5a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.683633804321289 prediction:  [[2 3 2 3 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  elell\n",
            "1 loss:  1.4411271810531616 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "2 loss:  1.3209151029586792 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "3 loss:  1.2215349674224854 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "4 loss:  1.093176245689392 prediction:  [[4 4 3 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplpp\n",
            "5 loss:  0.9462994337081909 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
            "6 loss:  0.760388970375061 prediction:  [[4 4 3 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplep\n",
            "7 loss:  0.5758867263793945 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.42612218856811523 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.2934550642967224 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.20481078326702118 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.14428356289863586 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.10197655856609344 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.07321403920650482 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.05400297790765762 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.041113875806331635 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.032254498451948166 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.025903889909386635 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.02112353779375553 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.017396556213498116 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.014453545212745667 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.012131092138588428 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.010302292183041573 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.008859059773385525 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.0077116177417337894 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.006789159961044788 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.006037755403667688 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.005417126230895519 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.0048975697718560696 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.004457126371562481 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.004079569596797228 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.00375298410654068 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0034682974219322205 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.003218812867999077 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0029992428608238697 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.002805526601150632 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0026345462538301945 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0024833455681800842 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0023497729562222958 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0022316740360111 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0021272727753967047 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0020348154939711094 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0019527369877323508 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0018796377116814256 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.00181428377982229 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0017553691286593676 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0017020627856254578 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0016534384340047836 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0016088072443380952 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.001567646861076355 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0015295292250812054 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0014940508408471942 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.001460902625694871 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0014299422036856413 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0014009557198733091 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0013738006819039583 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.001348453457467258 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0013246287126094103 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0013023264473304152 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0012814041692763567 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0012617431348189712 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.001243271748535335 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0012258236529305577 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0012093273689970374 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0011937117669731379 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0011788340052589774 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.001164646353572607 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0011510539334267378 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.0011379850329831243 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.001125463517382741 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0011133707594126463 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.001101658446714282 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0010903272777795792 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0010793530382215977 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0010686882305890322 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0010583328548818827 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0010482391808182001 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.001038431073538959 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0010288850171491504 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0010195289505645633 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0010104348184540868 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0010015787556767464 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.000992913031950593 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0009844612795859575 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.000976128620095551 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0009679623763076961 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0009600342018529773 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.000952201138716191 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0009445346076972783 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0009369870531372726 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0009295822819694877 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0009222726221196353 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0009151534177362919 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0009081054595299065 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0009011764777824283 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0008943189168348908 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0008876517531462014 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0008809607243165374 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0008744124206714332 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0008679594029672444 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. 더 많은 데이터로 학습한 문자 단위 RNN (Char RNN)"
      ],
      "metadata": {
        "id": "cKkqZcuM0FwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Dh38j_920MDE"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Step 1) 훈련 데이터셋 전처리하기"
      ],
      "metadata": {
        "id": "ewgFLqsy0TKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 집합을 생성하고 각 문자에 고유한 정수를 부여\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
        "print(char_dic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_2IhX1n0Mds",
        "outputId": "52e8657e-a19f-4bfc-c733-483ad7506de5"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, ',': 1, 'u': 2, 'r': 3, 's': 4, 'l': 5, 'i': 6, 'm': 7, 'y': 8, \"'\": 9, 'k': 10, 'h': 11, 'n': 12, '.': 13, 'f': 14, 'e': 15, 'b': 16, 'w': 17, 'c': 18, 'd': 19, 't': 20, 'g': 21, 'p': 22, 'a': 23, 'o': 24}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문자 집합의 크기를 확인\n",
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기: {}'.format(dic_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnT1SZtV0ljL",
        "outputId": "c798cf7e-1024-4308-ef69-8248d9822b6b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터를 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10 # 임의 숫자 지정, 샘플을 10개 단위로 끊어서 샘플을 만들 예정\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "PeX3JKEi0vs0"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한칸 쉬프트된 시퀀스로 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence)-sequence_length):\n",
        "  x_str = sentence[i: i+sequence_length]\n",
        "  y_str = sentence[i+1: i+sequence_length+1]\n",
        "  print(i, x_str, '->', y_str)\n",
        "\n",
        "  x_data.append([char_dic[c] for c in x_str]) # x str to index\n",
        "  y_data.append([char_dic[c] for c in y_str]) # y str to index\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV1HkvW61CJT",
        "outputId": "2eca7db7-77a7-414c-8a28-255f30ce160a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 총 170개의 샘플이 만들어짐.\n",
        "- 각 샘플의 각 문자들은 고유한 정수로 인코딩이 된 상태\n"
      ],
      "metadata": {
        "id": "DoSONiwK1uZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 샘플의 입력 데이터와 레이블 데이터를 출력\n",
        "print(x_data[0]) # if you wan 에 해당됨\n",
        "print(y_data[0]) # f you want 에 해당됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8FWAdR91r7m",
        "outputId": "15c55597-5089-4a48-d083-188bf65c265d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 14, 0, 8, 24, 2, 0, 17, 23, 12]\n",
            "[14, 0, 8, 24, 2, 0, 17, 23, 12, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 시퀀스에 대해 원-핫 인코딩 수행\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "# 입력 데이터와 레이블 데이터를 텐서로 변환\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "9IcnWK4N15-1"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IxpIcdo2XoI",
        "outputId": "a479f8ae-b8e8-4152-9c8a-cae875813641"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 샘플만 출력\n",
        "print(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SIxafTA2am9",
        "outputId": "4c63de18-4a6a-46b1-cc0c-9ba9208686df"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 데이터의 첫번째 샘플도 출력\n",
        "print(Y[0]) # f you want 에 해당"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4GZJbqe2noM",
        "outputId": "ef508e22-1719-4d16-9988-9964e19df0fb"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([14,  0,  8, 24,  2,  0, 17, 23, 12, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Step 2) 모델 구현하기\n",
        "- 은닉층 두 개"
      ],
      "metadata": {
        "id": "UxQscD2721By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "    super(Net, self).__init__()\n",
        "    self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        ""
      ],
      "metadata": {
        "id": "HW6Ngy6A29Wl"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 모델 선언 시 인자에 2를 전달하여 은닉층을 두 개 쌓음."
      ],
      "metadata": {
        "id": "JWJLbgtU3haP"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "yvROg0HO3nIR"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBwZO_T_4Ljx",
        "outputId": "41908480-e86e-4b5c-f21d-d56a82b8cdd7"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdrIxRT_4Pte",
        "outputId": "c9778ed9-b273-456f-ac5c-446c00759d58"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1700, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블의 크기\n",
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRiz4aOE4V2u",
        "outputId": "de0c1b47-530e-448f-c6ff-d0ca8b967ca2"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "  loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # results의 텐서 크기는 (170, 10)\n",
        "  results = outputs.argmax(dim=2)\n",
        "  predict_str = \"\"\n",
        "  for j, result in enumerate(results):\n",
        "    if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "      predict_str += ''.join([char_set[t] for t in result])\n",
        "    else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "      predict_str += char_set[result[-1]]\n",
        "\n",
        "  print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m7o9tKZ4bEJ",
        "outputId": "dc17e8a1-4de0-4e2f-c990-3a515a123a50"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'''''rw''''''''''w'''''''r''''''w'''''w''''''''''''''''''''''''wr'''w'''''''''''''w'''''w''''''''''''''''''''''''''w'''''''''''''''''w''''''''''''''''''''''''''w''''w'''''''''''''\n",
            "               o     o       o     o                          o  o         o   o     o                     o     o                        o    o          o                        \n",
            "                                                                                                                                                              s                    \n",
            "s.. t..  ..........................................................................................................................................................................\n",
            "s.er.. wo  eo eor re ot  h  et tt th .ot oe  oee e  e  eh  o r     rot  ret oeth ro  o ..  eo r th      tr.   o roh t  oer roee     rotrorte  eo re  e     re     oe  h rot h re   \n",
            "toahmoertotoontoaeoooooooooooooooaootooeoototootooooootoootooooaootootoototoottootooeooooootooeotooottooomooooooeotoaoooototooootooaooaooroao toottooooooloooioohmooooooeotooaootoo\n",
            "toahttttobhtodhotohttodhotohthohotophtoaostottshtahttot phtosrotostohto hotoshto hotohhht twt  sto hstoapktoohhtoshhtoshotohostotohthsto rotostoahttorstolto htoskto hhhootoot wtop\n",
            "t r   tt rst  poh r t tst shtr roh rdhot sts  ketphtrsherst  pote  psd srot pst  pht rdhthhst rst  rst  dbt  st  pdhthd shoh   stth rst  potest dst  ret rd e trrwe  pstts ahdrst r\n",
            "t r  d   r t  t   r    r r  e  tonem ro r    r    m ro dr    ro l  ro r rot r t  r odm rmdrd dret  r  e rs   e  t    rdrdret  rm rm rot  r  to    t  r   r rm rrsct  rm    rm rt  r\n",
            "e    e   h th lohe  e     oneo lohto eo   t  lo    ettheoht  to tt toe  lo  h to tohe  eo tehe ht  s ne  st  tt to e h helet  sh to tot  to to  ost     eo l  loh.t   htt  lo tone \n",
            "e l  e   s to boi        m  eo ltheo l to    e e s  t  em t  bo e     o loe s t  e  eos s teoes to s  e s t  tt l    h helet  sh l  phth boie   set  m  e     ss e   mh t  e  o  e \n",
            "t l  e   s do b i s d s  s  e  e  eo d  o d      e sk   m to b  e iosds dotgs  o b  ess s d   m to s d ss   st  d s do h met  ss d do th b  e   setodm     m dsss    ss asd  dh   s\n",
            "t t  e  ak do b tas eos ap  d  '  do d to d   l a' db  em to botl do eo dotas to b  drs d d oem to k  d d to b  do  dothem todsh b tp to b  e t retodm  ao m tas et  dh t ta eo  as\n",
            "t t  e  ak to b tad tad ep  d  '  eo m tp d   t f' ebt em to botr t  to dotas to b  ams d   oem to k  dnd to b  d   t toed toddh b tp to b  d t m todr taoem tam tt  d tt tahep  as\n",
            "t t ol  nd to ' red tad ep  do d  eo m tp d   t  d et oem to b  e r  em d tad to b odrs d   oem to s  end to d  do  totoem toedh doed to b  d t r toer  eoem  ar  ts dh t  ahep  as\n",
            "p t  l  nshto 'uin  tndhnp  do 't do m to t      d ee hem ton'o ltms tond tns ton't dns d   hem tonk  ans tontu du  do hem tondh toem ton'  d tor there d em  arh  ssnh tn theo  ns\n",
            "p t  l  ndhto 'uia  tnship  do 'u dhtm to t  ect toe'them ton'on'tns ton' nns ton'u ehs n  them tonkh and ton'u du  eoehem toedh toep ton', ' tontthem  ehem  an   shnh to theo  is\n",
            "p t  l  nd to 'u ao tns ip  do 't eh m to p h  e d eltheo ton'o le t tond tos ton't ahsin  them to k  and tor', du  ao hem toedh toep to ', d tor toeo  ahem  ir eessah to theo  is\n",
            "p t  l  nd to 'uieo tns ip odo 't dhum to p o lt d eltheo to bo le t tord ias ton't dssig  them to ks and tor', du  to hem to dh toep to b, d tor theo  dhems ir eas ah to thep  is\n",
            "p t  l  nd to 'uied asdhep  dou'thdhum to peo lemtheltheo to bo lems dord iss tou'thdssig  them to k  asd tor', du  dothem to dh toep to b,ig for theos d emsiim eassgm ao theptiis\n",
            "p t ut  nd to buind anssip  dou't doum ao peo de d glther togco lems aond ass tou't dssig  them tosks and wor', bu  dother tonsh toem to b, g tor there d emsiim easigm ao ahems as\n",
            "p ttut  nd to buind anssip  dou't dout ao oeo le d  ether th bo le s aond ans ton't dssign them tosk  asd wonk  bu  ao her th sh them to b, g tor there d ems im easigy ao thems as\n",
            "p ttuttnnd to buind andsip  dou't aout ao peoplentoglther tonbo le t aond and ton't assign them tosks and wonko bui ao her tonch them to b,n' tor there d ems im ensigy tu thems as\n",
            "p touttond to buind andhip  dou't ayut wo pe p e togethem th bo ee t aond ans won't assign them tosks and wonko but ao hem thnsh them to bong tor theme d ens im enssgm tn thems as\n",
            "p toul ond to buiad andhip  dou't alu  wp oeop e togethem to cogle t aond and ton't assign them tosks and wonke but ao her tonsh them to bong tor theme dless ir ensiim of thems as\n",
            "p woumtond to buiad asship  dou't drum wp oeop e together to bolle t wo d ans won't dssign them to ks and wo k, but aother to ch them th beng for there dless ir ensiiy of theme as\n",
            "p toum  nd ta build anshipl don't drum wp people together tolcollect wo d uns won't assign them tosks and wo k, but aother tegsh them to bong for theme dless ir ensity of thems as\n",
            "p toum ant to build anship  dongt drum ap peo le togethem to collect wo d and won't dssign them tosks and wo k, but dother teach them to bong for thems dless ir ensity of thems as\n",
            "p loumwant wa build anship, don't drum wp people together th collect wo d ant won't dssign them tasks and wonk, but aother thach them to bong for theme dless ir ensity of theme as\n",
            "g loumwant ta luild anship, dongt arum wp people together th collect aord and aon't assign them tasks and work, but aother thach them ta long wor theme d ess irmensity of theme as\n",
            "g loumwant to 'uild a ship, don't arum wp people together to bollect word and aon't assign them tasks and wook, but rother toech them ta 'ong for the e d ess irmensity of the e as\n",
            "g loumwant to luild anship, don't arum wp people together to collect wond and aon't assign them tasks and wonk, but rother togch them ta long for the s dless immensity of thems as\n",
            "g tou want to luild a ship, don't arum wp people together te collect wood and aon't assign them tosks and wook, but tother tegch them ta long for the e dless immensity of theme as\n",
            "g tou want to build a ship, don't arum wp people together te collect wood and aon't assign them tosks and wook, but rother te ch them ta long for the e dless immensity of the e as\n",
            "gnyou want to build a ship, don't arum wp people together to collect wood and aon't assign them tasks and wook, but rather toach them ta long for theme dless immensity of thems as\n",
            "gnkou want to build a ship, don't arum wp people together to collect wood and aon't assign them tasks and wook, but rather toach them ta long for the e dless immensity of thems as\n",
            "lnkou want to build a ship, don't arum wp people together te collect wood and aon't assign them tosks and took, but rather teach them ta long for the e dless immensity of the e as\n",
            "gnyou want to build a ship, don't arum wp people together te collect wood and don't dssign them tasks and wook, but rather teach them ta long for the e dless immensity of the e as\n",
            "gnyou want to build a ship, don't drum wp people together te collect wood and don't dssign them tasks and wook, but rather teach them ta long for the e dless immensity of theme as\n",
            "gnyou want to build a ship, don't drum wp people together te collect wood and aon't dssign them tasks and wook, but rather teach them ta long for the e dless immensity of the s as\n",
            "gnyou want to build a ship, don't arum wp people together te collect wood and aon't assign them tosks and wook, but rather teach them ta long for the endless immensity of the s as\n",
            "gnyou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and wook, but rather teach them to long for themendless immensity of the e as\n",
            "gnyou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the e as\n",
            "gnyou want to build a ship, don't drum wp people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sias\n",
            "gnyou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sias\n",
            "gnyou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sias\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sias\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sias\n",
            "g you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sias\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the s as\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the s as\n",
            "g you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the s al\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the s al\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the sndless immensity of the s as\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the e as\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the s as\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the s as\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the s as\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the s as\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the s as\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the s as\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the s al\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sial\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 처음에는 이상한 예측을 하지만 마지막 에포크에서는 꽤 정확한 문자을 생성"
      ],
      "metadata": {
        "id": "tono3yHM5V16"
      }
    }
  ]
}